{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "split-aluminum",
      "metadata": {
        "id": "split-aluminum",
        "papermill": {
          "duration": 0.048394,
          "end_time": "2021-04-15T21:06:39.560571",
          "exception": false,
          "start_time": "2021-04-15T21:06:39.512177",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# LAB | Extractive Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prospective-turner",
      "metadata": {
        "id": "prospective-turner",
        "papermill": {
          "duration": 0.045573,
          "end_time": "2021-04-15T21:06:39.651272",
          "exception": false,
          "start_time": "2021-04-15T21:06:39.605699",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "This notebook demonstrates how Pinecone helps you build an extractive question-answering application. To build an extractive question-answering system, we need three main components:\n",
        "\n",
        "- A vector index to store and run semantic search\n",
        "- A retriever model for embedding context passages\n",
        "- A reader model to extract answers\n",
        "\n",
        "We will use the SQuAD dataset, which consists of **questions** and **context** paragraphs containing question **answers**. We generate embeddings for the context passages using the retriever, index them in the vector database, and query with semantic search to retrieve the top k most relevant contexts containing potential answers to our question. We then use the reader model to extract the answers from the returned contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oC3GG-dWkZJ6",
      "metadata": {
        "id": "oC3GG-dWkZJ6"
      },
      "source": [
        "Let's get started by installing the packages needed for notebook to run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "haI-VR5hUfaT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haI-VR5hUfaT",
        "outputId": "0dfa6012-c5af-4b6d-8315-2f8bfd89e650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping pinecone-client as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pinecone in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2025.8.3)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from pinecone) (1.8.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: packaging<25.0,>=24.2 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.55.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall pinecone-client # Uninstall the old library first\n",
        "!pip install pinecone # Install the new official client\n",
        "!pip install sentence-transformers\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "m8W5N7FrzaKl",
      "metadata": {
        "id": "m8W5N7FrzaKl"
      },
      "outputs": [],
      "source": [
        "#!pip install pinecone-client\n",
        "#!pip install sentence-transformers\n",
        "#!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "740a3cf7-830c-4f1d-bf8d-90d18f908a99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "740a3cf7-830c-4f1d-bf8d-90d18f908a99",
        "outputId": "b3be40a0-083c-4d03-ed0b-2e966c9ca400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API keys loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
        "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
        "\n",
        "print(\"API keys loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "terminal-export",
      "metadata": {
        "id": "terminal-export",
        "papermill": {
          "duration": 0.044413,
          "end_time": "2021-04-15T21:06:39.741951",
          "exception": false,
          "start_time": "2021-04-15T21:06:39.697538",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "expressed-executive",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-15T21:06:39.845309Z",
          "iopub.status.busy": "2021-04-15T21:06:39.842494Z",
          "iopub.status.idle": "2021-04-15T21:08:22.163939Z",
          "shell.execute_reply": "2021-04-15T21:08:22.164616Z"
        },
        "id": "expressed-executive",
        "papermill": {
          "duration": 102.376674,
          "end_time": "2021-04-15T21:08:22.165052",
          "exception": false,
          "start_time": "2021-04-15T21:06:39.788378",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install -qU datasets pinecone-client sentence-transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29ad3840",
      "metadata": {
        "id": "29ad3840"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hgIieQukgagu",
      "metadata": {
        "id": "hgIieQukgagu"
      },
      "source": [
        "Now let's load the SQUAD dataset from the HuggingFace Model Hub. We load the dataset into a pandas dataframe and filter the title, question, and context columns, and we drop any duplicate context passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "J250IJeh7NIb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J250IJeh7NIb",
        "outputId": "159bbd77-3e2a-4c57-f7a5-2608be4d1f23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load the squad dataset into a pandas dataframe\n",
        "df = load_dataset(\"squad\", split=\"train\").to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "FcmeNO97dHDO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcmeNO97dHDO",
        "outputId": "07c6790b-98d1-41e2-d580-fbe78a03e255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       title  \\\n",
            "0   University_of_Notre_Dame   \n",
            "5   University_of_Notre_Dame   \n",
            "10  University_of_Notre_Dame   \n",
            "15  University_of_Notre_Dame   \n",
            "20  University_of_Notre_Dame   \n",
            "\n",
            "                                              context  \n",
            "0   Architecturally, the school has a Catholic cha...  \n",
            "5   As at most other universities, Notre Dame's st...  \n",
            "10  The university is the major seat of the Congre...  \n",
            "15  The College of Engineering was established in ...  \n",
            "20  All of Notre Dame's undergraduate students are...  \n"
          ]
        }
      ],
      "source": [
        "# select only title and context columns\n",
        "df = df.loc[:, ['title', 'context']]\n",
        "# or alternatively\n",
        "# df = df[['title', 'context']]\n",
        "\n",
        "# drop rows containing duplicate context passages\n",
        "df = df.drop_duplicates(subset=['context'])\n",
        "\n",
        "# display the head of the dataframe to confirm changes\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "IDgC1uyFBJw-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDgC1uyFBJw-",
        "outputId": "62247703-3b43-48ee-a903-740b8b7894b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: pinecone-client 6.0.0\n",
            "Uninstalling pinecone-client-6.0.0:\n",
            "  Successfully uninstalled pinecone-client-6.0.0\n",
            "Found existing installation: pinecone 7.3.0\n",
            "Uninstalling pinecone-7.3.0:\n",
            "  Successfully uninstalled pinecone-7.3.0\n",
            "Collecting pinecone\n",
            "  Using cached pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2025.8.3)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from pinecone) (1.8.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: packaging<25.0,>=24.2 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
            "Using cached pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "Installing collected packages: pinecone\n",
            "Successfully installed pinecone-7.3.0\n",
            "Pinecone packages have been updated.\n"
          ]
        }
      ],
      "source": [
        "# The following code will uninstall the old Pinecone client\n",
        "# and install the new one. This is a one-time fix.\n",
        "!pip uninstall -y pinecone-client pinecone\n",
        "!pip install --upgrade pinecone\n",
        "\n",
        "print(\"Pinecone packages have been updated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57bbcb57",
      "metadata": {
        "id": "57bbcb57"
      },
      "source": [
        "# Initialize Pinecone Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e24d904c",
      "metadata": {
        "id": "e24d904c"
      },
      "source": [
        "The Pinecone index stores vector representations of our context passages which we can retrieve using another vector (query vector). We first need to initialize our connection to Pinecone to create our vector index. For this, we need a free [API key](\"https://app.pinecone.io/\"), and then we initialize the connection like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5983bc0e",
      "metadata": {
        "id": "5983bc0e"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-pinecone pinecone-notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "092d1e71",
      "metadata": {
        "id": "092d1e71"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")\n",
        "\n",
        "# connect to pinecone environment\n",
        "pc = Pinecone(\n",
        "    api_key = PINECONE_API_KEY,\n",
        "    environment='us-east-1'  # find next to API key in console\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58028e12",
      "metadata": {
        "id": "58028e12"
      },
      "source": [
        "Now we create a new index called \"question-answering\" â€” we can name the index anything we want. We specify the metric type as \"cosine\" and dimension as 384 because the retriever we use to generate context embeddings is optimized for cosine similarity and outputs 384-dimension vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b3206184",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3206184",
        "outputId": "ec569b2c-3750-45ac-d654-c918a67b534f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index 'question-answering' already exists.\n"
          ]
        }
      ],
      "source": [
        "index_name = \"question-answering\"\n",
        "\n",
        "# check if the question-answering index exists\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # create the index if it does not exist\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    print(f\"Index '{index_name}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Index '{index_name}' already exists.\")\n",
        "\n",
        "# connect to the question-answering index we created\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e84a3e5",
      "metadata": {
        "id": "6e84a3e5"
      },
      "source": [
        "# Initialize Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oZzhGS1Lpj0g",
      "metadata": {
        "id": "oZzhGS1Lpj0g"
      },
      "source": [
        "Next, we need to initialize our retriever. The retriever will mainly do two things:\n",
        "\n",
        "- Generate embeddings for all context passages (context vectors/embeddings)\n",
        "- Generate embeddings for our questions (query vector/embedding)\n",
        "\n",
        "The retriever will generate embeddings in a way that the questions and context passages containing answers to our questions are nearby in the vector space. We can use cosine similarity to calculate the similarity between the query and context embeddings to find the context passages that contain potential answers to our question.\n",
        "\n",
        "We will use a SentenceTransformer model named ``multi-qa-MiniLM-L6-cos-v1`` designed for semantic search and trained on 215M (question, answer) pairs from diverse sources as our retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "31a85bb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31a85bb3",
        "outputId": "5914f111-5dea-4c2c-c328-1cf718bccb99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Retriever model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# set device to GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the retriever model from the Hugging Face model hub.\n",
        "# 'multi-qa-MiniLM-L6-cos-v1' is designed for semantic search and was trained on 215M\n",
        "# (question, answer) pairs to make queries and relevant context passages\n",
        "# close in vector space.\n",
        "retriever = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1', device=device)\n",
        "\n",
        "print(\"Retriever model loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "FEpVWoCXKeDF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEpVWoCXKeDF",
        "outputId": "5b4db9f0-f918-4c5c-bbfd-9f66eff46f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['title', 'context'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aaad0a2",
      "metadata": {
        "id": "8aaad0a2"
      },
      "source": [
        "# Generate Embeddings and Upsert"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hgy7AagJtO_p",
      "metadata": {
        "id": "Hgy7AagJtO_p"
      },
      "source": [
        "Next, we need to generate embeddings for the context passages. We will do this in batches to help us more quickly generate embeddings and upload them to the Pinecone index. When passing the documents to Pinecone, we need an id (a unique value), context embedding, and metadata for each document representing context passages in the dataset. The metadata is a dictionary containing data relevant to our embeddings, such as the article title, context passage, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a17824ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a17824ef",
        "outputId": "262e5dba-7609-4c10-e7f6-a506bd619d79",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Columns:\n",
            "Index(['title', 'context'], dtype='object')\n",
            "--------------------\n",
            "Using the correct column name 'title':\n",
            "The title of the first article is: First Article\n",
            "--------------------\n",
            "Columns after renaming:\n",
            "Index(['article_title', 'context'], dtype='object')\n",
            "--------------------\n",
            "The title of the first article (using the new name) is: First Article\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample DataFrame to replicate your issue\n",
        "data = {'title': ['First Article', 'Second Article'],\n",
        "        'context': ['This is the first article context.', 'This is the second article context.']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the columns to confirm the names, just as you did\n",
        "print(\"Original Columns:\")\n",
        "print(df.columns)\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Option 1: Access the correct column\n",
        "print(\"Using the correct column name 'title':\")\n",
        "first_article_title = df['title'][0]\n",
        "print(f\"The title of the first article is: {first_article_title}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Option 2: Rename the column\n",
        "# We use a dictionary to map the old name to the new name.\n",
        "df.rename(columns={'title': 'article_title'}, inplace=True)\n",
        "\n",
        "# Now, print the columns again to see the change\n",
        "print(\"Columns after renaming:\")\n",
        "print(df.columns)\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Now you can safely access the column using the new name\n",
        "first_article_title_renamed = df['article_title'][0]\n",
        "print(f\"The title of the first article (using the new name) is: {first_article_title_renamed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YFyBYafuJ0y0",
      "metadata": {
        "id": "YFyBYafuJ0y0"
      },
      "source": [
        "# Initialize Reader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HgdiLCz5ynOk",
      "metadata": {
        "id": "HgdiLCz5ynOk"
      },
      "source": [
        "We use the `deepset/electra-base-squad2` model from the HuggingFace model hub as our reader model. We load this model into a \"question-answering\" pipeline from HuggingFace transformers and feed it our questions and context passages individually. The model gives a prediction for each context we pass through the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "hg9XTDkIJzH_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg9XTDkIJzH_",
        "outputId": "8a6af68c-1bd5-494d-9d69-e407d536beb5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x7b26a16d67e0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_name = 'deepset/electra-base-squad2'\n",
        "# load the reader model into a question-answering pipeline\n",
        "reader = pipeline(tokenizer=model_name, model=model_name, task='question-answering', device=device)\n",
        "reader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14d89d6",
      "metadata": {
        "id": "e14d89d6"
      },
      "source": [
        "Now all the components we need are ready. Let's write some helper functions to execute our queries. The `get_context` function retrieves the context embeddings containing answers to our question from the Pinecone index, and the `extract_answer` function extracts the answers from these context passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "lyYaY3QEQiHZ",
      "metadata": {
        "id": "lyYaY3QEQiHZ"
      },
      "outputs": [],
      "source": [
        "# This function gets context passages from the Pinecone index\n",
        "def get_context(question, top_k):\n",
        "    \"\"\"\n",
        "    Generates embeddings for a question and searches a Pinecone index\n",
        "    to find the most relevant context passages.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question string.\n",
        "        top_k (int): The number of top context passages to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of context passages (strings).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Generate embeddings for the question using the embedder model.\n",
        "    # NOTE: You must have an 'embedder' object defined elsewhere in your code.\n",
        "    # This object is typically a HuggingFace SentenceTransformer or similar model.\n",
        "    # Example: from sentence_transformers import SentenceTransformer\n",
        "    #          embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    xq = embedder.encode(question).tolist()\n",
        "\n",
        "    # 2. Search the Pinecone index for context passages with the answer.\n",
        "    # NOTE: You must have an 'index' object from Pinecone initialized elsewhere.\n",
        "    # Example: import pinecone\n",
        "    #          index = pinecone.Index('your-index-name')\n",
        "    xc = index.query(xq, top_k=top_k, include_metadata=True)\n",
        "\n",
        "    # 3. Extract the context passages from the Pinecone search result.\n",
        "    c = [match['metadata']['text'] for match in xc['matches']]\n",
        "\n",
        "    return c\n",
        "\n",
        "\n",
        "# This function extracts the answer from the context using the reader model\n",
        "def extract_answer(question, context_list):\n",
        "    \"\"\"\n",
        "    Extracts an answer for a question from a list of context passages\n",
        "    using a question-answering pipeline.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question string.\n",
        "        context_list (list): A list of context passages (strings).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the answer, score, start, and end.\n",
        "    \"\"\"\n",
        "    # NOTE: This function assumes you have a 'reader' pipeline defined,\n",
        "    # as shown in your previous code cell.\n",
        "\n",
        "    # The pipeline works best with a single context. For a list of contexts,\n",
        "    # we'll extract the best answer by feeding each context to the reader\n",
        "    # and selecting the one with the highest score.\n",
        "\n",
        "    best_answer = None\n",
        "    max_score = -1.0\n",
        "\n",
        "    for context in context_list:\n",
        "        prediction = reader(question=question, context=context)\n",
        "\n",
        "        # Check if this prediction is better than the current best\n",
        "        if prediction['score'] > max_score:\n",
        "            best_answer = prediction\n",
        "            max_score = prediction['score']\n",
        "\n",
        "    return best_answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "Dc9VYOiUQA7B",
      "metadata": {
        "id": "Dc9VYOiUQA7B"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# This function extracts the answer from the context passage(s) and returns the best one\n",
        "def extract_answer(question, context):\n",
        "    \"\"\"\n",
        "    Feeds a question and a list of contexts to a question-answering reader model,\n",
        "    extracts the best answer, and prints all results sorted by confidence score.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question string.\n",
        "        context (list): A list of context passages (strings).\n",
        "\n",
        "    Returns:\n",
        "        list: The list of all results (answers with their scores and contexts),\n",
        "              sorted from highest to lowest score.\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "    for c in context:\n",
        "        # Feed the reader the question and a single context to extract an answer\n",
        "        answer = reader(question=question, context=c)\n",
        "\n",
        "        # Add the context to the answer dictionary for printing both together\n",
        "        answer[\"context\"] = c\n",
        "        results.append(answer)\n",
        "\n",
        "    # Sort the results based on the score from the reader model, from highest to lowest\n",
        "    sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    # Print the sorted results for easy viewing\n",
        "    print(\"All Answers Sorted by Confidence Score:\")\n",
        "    pprint(sorted_results)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Return the sorted list so you can use it later\n",
        "    return sorted_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5E3a3dkJ5ZQD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E3a3dkJ5ZQD",
        "outputId": "8a463056-fb68-48fe-c661-f698418596a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Pinecone client...\n",
            "Successfully connected to index 'question-answering'.\n",
            "\n",
            "Upserting sample vectors...\n",
            "Vectors upserted successfully.\n",
            "\n",
            "Querying for the most similar vectors...\n",
            "Query results:\n",
            "  ID: vector1\n",
            "  Score: 0.999999\n",
            "  Text: What is the capital of France?\n",
            "  ID: vector3\n",
            "  Score: 0.885088444\n",
            "  Text: What are the benefits of using Pinecone?\n",
            "\n",
            "Example complete.\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import time\n",
        "\n",
        "# You should use environment variables for your API key in a real application.\n",
        "# For this example, replace \"YOUR_API_KEY\" with your actual key.\n",
        "API_KEY = \"pcsk_2Jdn1q_UrtUYBaA95hb4QxQuxGXV61vDS9iXMjzvpUpo4EPrenEYxgQbnjnDkVD3Qvpvjo\"\n",
        "\n",
        "# This is the name of your index from the Pinecone dashboard.\n",
        "INDEX_NAME = \"question-answering\"\n",
        "\n",
        "# 1. Initialize the Pinecone client\n",
        "print(\"Initializing Pinecone client...\")\n",
        "pc = Pinecone(api_key=API_KEY)\n",
        "\n",
        "# 2. Check if the index exists and create it if necessary\n",
        "# The correct way to check is to get the list of index names.\n",
        "existing_indexes = [index.name for index in pc.list_indexes()]\n",
        "\n",
        "if INDEX_NAME not in existing_indexes:\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating it now...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=384,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",\n",
        "            region=\"us-east-1\"\n",
        "        )\n",
        "    )\n",
        "    # Wait for the index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "print(f\"Successfully connected to index '{INDEX_NAME}'.\")\n",
        "\n",
        "# 3. Connect to the index\n",
        "index = pc.Index(INDEX_NAME)\n",
        "\n",
        "# 4. Upsert (insert) some sample data\n",
        "# The dimension of the vector is 384. We create a helper function for clarity.\n",
        "def create_vector_values(base_list):\n",
        "    \"\"\"Repeats a small list to create a 384-dimensional vector.\"\"\"\n",
        "    repeated_list = base_list * (384 // len(base_list))\n",
        "    # Add any remaining elements to match the exact dimension\n",
        "    remaining_elements = 384 % len(base_list)\n",
        "    repeated_list.extend(base_list[:remaining_elements])\n",
        "    return repeated_list\n",
        "\n",
        "sample_vectors = [\n",
        "    {\n",
        "        \"id\": \"vector1\",\n",
        "        \"values\": create_vector_values([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
        "        \"metadata\": {\"text\": \"What is the capital of France?\"}\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"vector2\",\n",
        "        \"values\": create_vector_values([1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]),\n",
        "        \"metadata\": {\"text\": \"How does a vector database work?\"}\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"vector3\",\n",
        "        \"values\": create_vector_values([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),\n",
        "        \"metadata\": {\"text\": \"What are the benefits of using Pinecone?\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\nUpserting sample vectors...\")\n",
        "index.upsert(vectors=sample_vectors)\n",
        "print(\"Vectors upserted successfully.\")\n",
        "\n",
        "# 5. Query the index\n",
        "# This is a query vector you would get from a user's input.\n",
        "# It should have the same dimensions (384) as the vectors you stored.\n",
        "query_vector = create_vector_values([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
        "\n",
        "print(\"\\nQuerying for the most similar vectors...\")\n",
        "query_results = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=2,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "# 6. Print the results\n",
        "print(\"Query results:\")\n",
        "for match in query_results.matches:\n",
        "    print(f\"  ID: {match.id}\")\n",
        "    print(f\"  Score: {match.score}\")\n",
        "    print(f\"  Text: {match.metadata.get('text')}\")\n",
        "\n",
        "print(\"\\nExample complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "heKNVbWQ_LtC",
      "metadata": {
        "id": "heKNVbWQ_LtC"
      },
      "source": [
        "As we can see, the retiever is working fine and gets us the context passage that contains the answer to our question. Now let's use the reader to extract the exact answer from the context passage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "jUDuxJOEqrwc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUDuxJOEqrwc",
        "outputId": "2fdb0d3b-458e-4475-c7b6-bff074aaf8f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully connected to Pinecone.\n",
            "\n",
            "Indexes visible to this client:\n",
            "- support-chat-index\n",
            "- question-answering\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pinecone import Pinecone\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    api_key = userdata.get('PINECONE_API_KEY')\n",
        "    if not api_key:\n",
        "        print(\"Error: Pinecone API key not found in secrets.\")\n",
        "    else:\n",
        "        pc = Pinecone(api_key=api_key)\n",
        "        print(\"Successfully connected to Pinecone.\")\n",
        "\n",
        "        # Get and print all index names\n",
        "        index_names = pc.list_indexes().names()\n",
        "        print(\"\\nIndexes visible to this client:\")\n",
        "        if index_names:\n",
        "            for name in index_names:\n",
        "                print(f\"- {name}\")\n",
        "        else:\n",
        "            print(\"No indexes found.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5cRjIcR9Wm6p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cRjIcR9Wm6p",
        "outputId": "93377a32-9212-4f49-94c6-a9f667ad0d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding and upserting data...\n",
            "Data upserted successfully.\n",
            "Waiting for data to be indexed...\n",
            "\n",
            "Retrieved Context (from Pinecone):\n",
            "Mars is the fourth planet from the Sun and the second-smallest planet in the Solar System. It is often referred to as the 'Red Planet' due to the iron oxide prevalent on its surface.\n",
            "\n",
            "Using the reader to extract the exact answer...\n",
            "\n",
            "Final Answer:\n",
            "Mars is known as the 'Red Planet'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pinecone\n",
        "import json\n",
        "import time # We'll need this library for the fix\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from requests import post\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- Configuration ---\n",
        "# Your API keys must be set as secrets in this Colab notebook.\n",
        "# Go to the key icon on the left-hand sidebar, add a new secret, and\n",
        "# name them PINECONE_API_KEY and GEMINI_API_KEY.\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# Check if the API keys have been set.\n",
        "if not pinecone_api_key or not gemini_api_key:\n",
        "    print(\"Error: API keys not found.\")\n",
        "    print(\"Please set your PINECONE_API_KEY and GEMINI_API_KEY as secrets in this Colab notebook.\")\n",
        "    exit()\n",
        "\n",
        "# Initialize Pinecone client\n",
        "try:\n",
        "    # Use the API key from environment variables\n",
        "    pc = Pinecone(api_key=pinecone_api_key)\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Pinecone: {e}\")\n",
        "    exit()\n",
        "\n",
        "# We will use a small, efficient embedding model from Hugging Face\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name)\n",
        "# NOTE: This model creates vectors with a dimension of 384. Your Pinecone index\n",
        "# MUST be created with this exact dimension.\n",
        "\n",
        "# Define your Pinecone index name. Make sure this matches the name of your index in the Pinecone console.\n",
        "# If you have an existing index, replace 'my-first-index' with its exact name.\n",
        "index_name = \"question-answering\"\n",
        "\n",
        "# NOTE: The check below now works because we've fixed the API key issue.\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    print(f\"Index '{index_name}' does not exist. Please create it first.\")\n",
        "    print(\"Dimensions should be 384 for the 'all-MiniLM-L6-v2' model.\")\n",
        "    exit()\n",
        "\n",
        "# Connect to the existing index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# --- Step 1: Prepare and Embed Text Data ---\n",
        "# These are the documents the LLM will \"read\" from.\n",
        "documents = [\n",
        "    \"The sun is the star at the center of the Solar System. It is by far the most important source of energy for life on Earth.\",\n",
        "    \"Mars is the fourth planet from the Sun and the second-smallest planet in the Solar System. It is often referred to as the 'Red Planet' due to the iron oxide prevalent on its surface.\",\n",
        "    \"The moon is Earth's only natural satellite. It is the fifth-largest satellite in the Solar System.\",\n",
        "    \"Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass more than two and a half times that of all the other planets in the Solar System combined.\"\n",
        "]\n",
        "\n",
        "print(\"Embedding and upserting data...\")\n",
        "embeddings = model.encode(documents).tolist()\n",
        "vectors_to_upsert = [\n",
        "    (str(i), embeddings[i], {\"text\": documents[i]}) for i in range(len(documents))\n",
        "]\n",
        "index.upsert(vectors=vectors_to_upsert)\n",
        "print(\"Data upserted successfully.\")\n",
        "# Add a short delay to give the index time to process the data\n",
        "print(\"Waiting for data to be indexed...\")\n",
        "time.sleep(5) # Pause for 5 seconds\n",
        "\n",
        "# --- Step 2: Implement the Reader Function ---\n",
        "def extract_answer(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses a large language model to extract a precise answer from a context.\n",
        "\n",
        "    Args:\n",
        "        question: The user's question.\n",
        "        context: The text retrieved by the retriever to use as a basis for the answer.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the extracted answer.\n",
        "    \"\"\"\n",
        "    if not gemini_api_key:\n",
        "        return \"Error: Gemini API key not found. Please set it as a secret.\"\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={gemini_api_key}\"\n",
        "\n",
        "    # Craft the prompt to guide the LLM's behavior\n",
        "    prompt = f\"\"\"\n",
        "    Answer the following question only based on the provided context. If the answer is not found in the context, respond with \"Answer not found.\".\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"tools\": [{\"google_search\": {}}]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = post(url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload))\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "        result = response.json()\n",
        "        candidate = result.get(\"candidates\", [{}])[0]\n",
        "        text = candidate.get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Answer not found.\")\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Gemini API: {e}\")\n",
        "        return \"Answer not found due to an API error.\"\n",
        "\n",
        "# --- Step 3: Perform RAG with the Reader ---\n",
        "# Define a question and use the retriever to find the most relevant document\n",
        "question = \"What planet is known as the 'Red Planet'?\"\n",
        "query_vector = model.encode(question).tolist()\n",
        "search_results = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=1, # We only need the single most relevant document for this example\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "if search_results.matches:\n",
        "    # Get the top result and its context. This is the crucial step.\n",
        "    context = search_results.matches[0].metadata[\"text\"]\n",
        "\n",
        "    print(\"\\nRetrieved Context (from Pinecone):\")\n",
        "    print(context)\n",
        "\n",
        "    # Now, the 'context' variable is defined and can be used.\n",
        "    print(\"\\nUsing the reader to extract the exact answer...\")\n",
        "    final_answer = extract_answer(question, context)\n",
        "\n",
        "    print(\"\\nFinal Answer:\")\n",
        "    print(final_answer)\n",
        "else:\n",
        "    print(\"No relevant context found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "DQ4GWdbMSjPl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DQ4GWdbMSjPl",
        "outputId": "3c53da25-d215-4df8-c16d-8264efab6bbd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Mars is known as the 'Red Planet'.\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_answer(question, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fMD_ABuDAyhN",
      "metadata": {
        "id": "fMD_ABuDAyhN"
      },
      "source": [
        "The reader model predicted with 99% accuracy the correct answer *691,000 bbl/d* as seen from the context passage. Let's run few more queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "F3oLl8-bK5O8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3oLl8-bK5O8",
        "outputId": "52578b7c-8bb8-4f7c-96f4-8f3435d59a20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding and upserting data...\n",
            "Data upserted successfully.\n",
            "\n",
            "Querying with 'What planet is known as the 'Red Planet'?'...\n",
            "\n",
            "Retrieved Context (from Pinecone):\n",
            "Mars is the fourth planet from the Sun and the second-smallest planet in the Solar System. It is often referred to as the 'Red Planet' due to the iron oxide prevalent on its surface.\n",
            "\n",
            "Using the reader to extract the exact answer...\n",
            "\n",
            "Final Answer:\n",
            "Mars is known as the 'Red Planet'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pinecone\n",
        "import json\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from requests import post\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- Configuration ---\n",
        "# Use Colab secrets to get API keys\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "if not pinecone_api_key or not gemini_api_key:\n",
        "    print(\"Error: API keys not found in Colab secrets.\")\n",
        "    exit()\n",
        "\n",
        "# Initialize Pinecone client and check for a valid index\n",
        "index = None # Initialize index to None to prevent NameError\n",
        "try:\n",
        "    pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "    # We will use a small, efficient embedding model from Hugging Face\n",
        "    model_name = 'all-MiniLM-L6-v2'\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    # Define your Pinecone index name.\n",
        "    index_name = \"question-answering\" # Make sure this matches the name of your index.\n",
        "\n",
        "    # Corrected method call: pc.list_indexes().names() is the correct way\n",
        "    if index_name not in pc.list_indexes().names():\n",
        "        print(f\"Index '{index_name}' does not exist. Please create it first.\")\n",
        "        print(\"Dimensions should be 384 for the 'all-MiniLM-L6-v2' model.\")\n",
        "        exit()\n",
        "\n",
        "    # Connect to the existing index\n",
        "    index = pc.Index(index_name)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Pinecone or connecting to the index: {e}\")\n",
        "    print(\"Please ensure your Pinecone API key and index name are correct, and that the index exists.\")\n",
        "\n",
        "\n",
        "# This check prevents the rest of the script from running if the index is not defined\n",
        "if index is None:\n",
        "    exit()\n",
        "\n",
        "# --- Step 2: Prepare and Embed Text Data ---\n",
        "# These are the documents the LLM will \"read\" from.\n",
        "documents = [\n",
        "    \"The sun is the star at the center of the Solar System. It is by far the most important source of energy for life on Earth.\",\n",
        "    \"Mars is the fourth planet from the Sun and the second-smallest planet in the Solar System. It is often referred to as the 'Red Planet' due to the iron oxide prevalent on its surface.\",\n",
        "    \"The moon is Earth's only natural satellite. It is the fifth-largest satellite in the Solar System.\",\n",
        "    \"Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass more than two and a half times that of all the other planets in the Solar System combined.\"\n",
        "]\n",
        "\n",
        "print(\"Embedding and upserting data...\")\n",
        "embeddings = model.encode(documents).tolist()\n",
        "vectors_to_upsert = [\n",
        "    (str(i), embeddings[i], {\"text\": documents[i]}) for i in range(len(documents))\n",
        "]\n",
        "index.upsert(vectors=vectors_to_upsert)\n",
        "print(\"Data upserted successfully.\")\n",
        "\n",
        "# --- Step 3: Implement the Reader Function ---\n",
        "def extract_answer(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses a large language model to extract a precise answer from a context.\n",
        "    \"\"\"\n",
        "    if not gemini_api_key:\n",
        "        return \"Please provide a valid Gemini API key to use the reader.\"\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={gemini_api_key}\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Answer the following question only based on the provided context. If the answer is not found in the context, respond with \"Answer not found.\".\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"tools\": [{\"google_search\": {}}]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = post(url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "\n",
        "        result = response.json()\n",
        "        candidate = result.get(\"candidates\", [{}])[0]\n",
        "        text = candidate.get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Answer not found.\")\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Gemini API: {e}\")\n",
        "        return \"Answer not found due to an API error.\"\n",
        "\n",
        "# --- Step 4: Perform RAG with the Reader ---\n",
        "query_text = \"What planet is known as the 'Red Planet'?\"\n",
        "\n",
        "print(f\"\\nQuerying with '{query_text}'...\")\n",
        "\n",
        "# Use the retriever to find the most relevant document\n",
        "query_vector = model.encode(query_text).tolist()\n",
        "search_results = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=1, # We only need the single most relevant document for this example\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "if search_results.matches:\n",
        "    # Get the top result and its context\n",
        "    top_match = search_results.matches[0]\n",
        "    retrieved_context = top_match.metadata[\"text\"]\n",
        "\n",
        "    print(\"\\nRetrieved Context (from Pinecone):\")\n",
        "    print(retrieved_context)\n",
        "\n",
        "    # Use the reader to extract the final answer\n",
        "    print(\"\\nUsing the reader to extract the exact answer...\")\n",
        "    final_answer = extract_answer(query_text, retrieved_context)\n",
        "\n",
        "    print(\"\\nFinal Answer:\")\n",
        "    print(final_answer)\n",
        "else:\n",
        "    print(\"No relevant context found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ZDhmTT-aPnMT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDhmTT-aPnMT",
        "outputId": "c3980372-687c-40ce-dbe1-22857fd6327d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Querying with 'What are the first names of the men that invented youtube?'...\n",
            "\n",
            "Retrieved Context (from Pinecone):\n",
            "What is the capital of France?\n",
            "\n",
            "Using the reader to extract the exact answer...\n",
            "\n",
            "Final Answer:\n",
            "Answer not found.\n"
          ]
        }
      ],
      "source": [
        "# --- Step 4: Perform RAG with the Reader ---\n",
        "query_text = \"What are the first names of the men that invented youtube?\"\n",
        "\n",
        "print(f\"\\nQuerying with '{query_text}'...\")\n",
        "\n",
        "# Use the retriever to find the most relevant document\n",
        "query_vector = model.encode(query_text).tolist()\n",
        "search_results = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=1, # We only need the single most relevant document for this example\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "if search_results.matches:\n",
        "    # Get the top result and its context\n",
        "    top_match = search_results.matches[0]\n",
        "    retrieved_context = top_match.metadata[\"text\"]\n",
        "\n",
        "    print(\"\\nRetrieved Context (from Pinecone):\")\n",
        "    print(retrieved_context)\n",
        "\n",
        "    # Use the reader to extract the final answer\n",
        "    print(\"\\nUsing the reader to extract the exact answer...\")\n",
        "    final_answer = extract_answer(query_text, retrieved_context)\n",
        "\n",
        "    print(\"\\nFinal Answer:\")\n",
        "    print(final_answer)\n",
        "else:\n",
        "    print(\"No relevant context found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "_vmWsr_3P7kV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vmWsr_3P7kV",
        "outputId": "dcd21a44-58d7-4f69-ac1c-3fe41fad5ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Querying with 'What is Albert Einstein famous for?'...\n",
            "\n",
            "Retrieved Context (from Pinecone):\n",
            "The sun is the star at the center of the Solar System. It is by far the most important source of energy for life on Earth.\n",
            "\n",
            "Using the reader to extract the exact answer...\n",
            "\n",
            "Final Answer:\n",
            "Answer not found.\n"
          ]
        }
      ],
      "source": [
        "# --- Step 4: Perform RAG with the Reader ---\n",
        "query_text = \"What is Albert Einstein famous for?\"\n",
        "\n",
        "print(f\"\\nQuerying with '{query_text}'...\")\n",
        "\n",
        "# Use the retriever to find the most relevant document\n",
        "query_vector = model.encode(query_text).tolist()\n",
        "search_results = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=1, # We only need the single most relevant document for this example\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "if search_results.matches:\n",
        "    # Get the top result and its context\n",
        "    top_match = search_results.matches[0]\n",
        "    retrieved_context = top_match.metadata[\"text\"]\n",
        "\n",
        "    print(\"\\nRetrieved Context (from Pinecone):\")\n",
        "    print(retrieved_context)\n",
        "\n",
        "    # Use the reader to extract the final answer\n",
        "    print(\"\\nUsing the reader to extract the exact answer...\")\n",
        "    final_answer = extract_answer(query_text, retrieved_context)\n",
        "\n",
        "    print(\"\\nFinal Answer:\")\n",
        "    print(final_answer)\n",
        "else:\n",
        "    print(\"No relevant context found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OhCgeny_BVno",
      "metadata": {
        "id": "OhCgeny_BVno"
      },
      "source": [
        "Let's run another question. This time for top 3 context passages from the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "RWKIvLndQXJk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWKIvLndQXJk",
        "outputId": "6a102719-4fe9-4241-e061-fed7d67baf02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Querying with 'Who was the first person to step foot on the moon?'...\n",
            "\n",
            "Retrieved Contexts (from Pinecone):\n",
            "The moon is Earth's only natural satellite. It is the fifth-largest satellite in the Solar System.\n",
            "The sun is the star at the center of the Solar System. It is by far the most important source of energy for life on Earth.\n",
            "Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass more than two and a half times that of all the other planets in the Solar System combined.\n",
            "\n",
            "Using the reader to extract the exact answer...\n",
            "\n",
            "Final Answer:\n",
            "Answer not found.\n"
          ]
        }
      ],
      "source": [
        "# --- Step 4: Perform RAG with the Reader ---\n",
        "query_text = \"Who was the first person to step foot on the moon?\"\n",
        "\n",
        "print(f\"\\nQuerying with '{query_text}'...\")\n",
        "\n",
        "# Use the retriever to find the most relevant documents (top_k=3)\n",
        "query_vector = model.encode(query_text).tolist()\n",
        "search_results = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=3, # Now retrieving the top 3 most similar documents\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "if search_results.matches:\n",
        "    # Get the top results and combine their contexts\n",
        "    retrieved_contexts = [match.metadata[\"text\"] for match in search_results.matches]\n",
        "    full_context = \"\\n\".join(retrieved_contexts)\n",
        "\n",
        "    print(\"\\nRetrieved Contexts (from Pinecone):\")\n",
        "    print(full_context)\n",
        "\n",
        "    # Use the reader to extract the final answer\n",
        "    print(\"\\nUsing the reader to extract the exact answer...\")\n",
        "    final_answer = extract_answer(query_text, full_context)\n",
        "\n",
        "    print(\"\\nFinal Answer:\")\n",
        "    print(final_answer)\n",
        "else:\n",
        "    print(\"No relevant context found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9oCWHy0tPpV",
      "metadata": {
        "id": "c9oCWHy0tPpV"
      },
      "source": [
        "The result looks pretty good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9--psyEjRCi9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9--psyEjRCi9",
        "outputId": "9bb31492-62f9-4d6e-c6d2-3fd50b8dd264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Error deleting index 'question-answering': (404)\n",
            "Reason: Not Found\n",
            "HTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': 'dda5b690ef884e1aec24fe469125be09', 'date': 'Tue, 02 Sep 2025 16:47:28 GMT', 'server': 'Google Frontend', 'Content-Length': '93', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
            "HTTP response body: {\"error\":{\"code\":\"NOT_FOUND\",\"message\":\"Resource question-answering not found\"},\"status\":404}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Step 5: Clean Up ---\n",
        "# This command will delete the Pinecone index to free up your resources.\n",
        "try:\n",
        "    pc.delete_index(index_name)\n",
        "    print(f\"\\nIndex '{index_name}' deleted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError deleting index '{index_name}': {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27fecadc-6976-43b2-90e1-788a8633ecc7",
      "metadata": {
        "id": "27fecadc-6976-43b2-90e1-788a8633ecc7"
      },
      "source": [
        "### Add a few more questions. What did you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "Jji-TjiFR3QL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jji-TjiFR3QL",
        "outputId": "a08db3fb-4135-4931-a47c-eaf5fb9be1ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index 'question-answering' does not exist. Creating it now...\n",
            "Waiting for index to be ready...\n",
            "Index is ready.\n",
            "Embedding and upserting data...\n",
            "Data upserted successfully.\n",
            "\n",
            "--- Processing new question: 'What is the star at the center of the solar system?' ---\n",
            "No relevant context found.\n",
            "\n",
            "--- Processing new question: 'Which is the fifth-largest satellite in the Solar System?' ---\n",
            "No relevant context found.\n",
            "\n",
            "--- Processing new question: 'What is the capital of Russia?' ---\n",
            "No relevant context found.\n",
            "\n",
            "--- Processing new question: 'What is Jupiter famous for?' ---\n",
            "No relevant context found.\n",
            "\n",
            "--- Processing new question: 'What color is Mars?' ---\n",
            "No relevant context found.\n",
            "\n",
            "--- Processing new question: 'What are the phases of the moon?' ---\n",
            "No relevant context found.\n",
            "\n",
            "Index 'question-answering' deleted successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pinecone\n",
        "import json\n",
        "import time\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from requests import post\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- Configuration ---\n",
        "# Use Colab secrets to get API keys\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "if not pinecone_api_key or not gemini_api_key:\n",
        "    print(\"Error: API keys not found in Colab secrets.\")\n",
        "    exit()\n",
        "\n",
        "# Initialize Pinecone client and check for a valid index\n",
        "index = None # Initialize index to None to prevent NameError\n",
        "try:\n",
        "    pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "    # We will use a small, efficient embedding model from Hugging Face\n",
        "    model_name = 'all-MiniLM-L6-v2'\n",
        "    model = SentenceTransformer(model_name)\n",
        "    dimension = model.get_sentence_embedding_dimension()\n",
        "\n",
        "    # Define your Pinecone index name.\n",
        "    index_name = \"question-answering\" # Make sure this matches the name of your index.\n",
        "\n",
        "    # Check if index exists, and create it if it doesn't\n",
        "    if index_name not in pc.list_indexes().names():\n",
        "        print(f\"Index '{index_name}' does not exist. Creating it now...\")\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=dimension,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "        )\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        while not pc.describe_index(index_name).status['ready']:\n",
        "            time.sleep(1)\n",
        "        print(\"Index is ready.\")\n",
        "\n",
        "    # Connect to the existing or newly created index\n",
        "    index = pc.Index(index_name)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Pinecone or connecting to the index: {e}\")\n",
        "    print(\"Please ensure your Pinecone API key and index name are correct, and that the index exists.\")\n",
        "    # Exit here if Pinecone fails to initialize\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Step 2: Prepare and Embed Text Data ---\n",
        "# These are the documents the LLM will \"read\" from.\n",
        "documents = [\n",
        "    \"The sun is the star at the center of the Solar System. It is by far the most important source of energy for life on Earth.\",\n",
        "    \"Mars is the fourth planet from the Sun and the second-smallest planet in the Solar System. It is often referred to as the 'Red Planet' due to the iron oxide prevalent on its surface.\",\n",
        "    \"The moon is Earth's only natural satellite. It is the fifth-largest satellite in the Solar System.\",\n",
        "    \"Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass more than two and a half times that of all the other planets in the Solar System combined.\"\n",
        "]\n",
        "\n",
        "print(\"Embedding and upserting data...\")\n",
        "embeddings = model.encode(documents).tolist()\n",
        "vectors_to_upsert = [\n",
        "    (str(i), embeddings[i], {\"text\": documents[i]}) for i in range(len(documents))\n",
        "]\n",
        "index.upsert(vectors=vectors_to_upsert)\n",
        "print(\"Data upserted successfully.\")\n",
        "\n",
        "# --- Step 3: Implement the Reader Function ---\n",
        "def extract_answer(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses a large language model to extract a precise answer from a context.\n",
        "    \"\"\"\n",
        "    if not gemini_api_key:\n",
        "        return \"Please provide a valid Gemini API key to use the reader.\"\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={gemini_api_key}\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Answer the following question only based on the provided context. If the answer is not found in the context, respond with \"Answer not found.\".\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"tools\": [{\"google_search\": {}}]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = post(url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "\n",
        "        result = response.json()\n",
        "        candidate = result.get(\"candidates\", [{}])[0]\n",
        "        text = candidate.get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Answer not found.\")\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Gemini API: {e}\")\n",
        "        return \"Answer not found due to an API error.\"\n",
        "\n",
        "# --- Step 4: Perform RAG with the Reader ---\n",
        "# Define a list of questions to run\n",
        "questions = [\n",
        "    \"What is the star at the center of the solar system?\",\n",
        "    \"Which is the fifth-largest satellite in the Solar System?\",\n",
        "    \"What is the capital of Russia?\",\n",
        "    \"What is Jupiter famous for?\",\n",
        "    \"What color is Mars?\",\n",
        "    \"What are the phases of the moon?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"\\n--- Processing new question: '{question}' ---\")\n",
        "\n",
        "    # Use the retriever to find the most relevant documents (top_k=3)\n",
        "    query_vector = model.encode(question).tolist()\n",
        "    search_results = index.query(\n",
        "        vector=query_vector,\n",
        "        top_k=3, # Now retrieving the top 3 most similar documents\n",
        "        include_metadata=True\n",
        "    )\n",
        "\n",
        "    if search_results.matches:\n",
        "        # Get the top results and combine their contexts\n",
        "        retrieved_contexts = [match.metadata[\"text\"] for match in search_results.matches]\n",
        "        full_context = \"\\n\".join(retrieved_contexts)\n",
        "\n",
        "        print(\"\\nRetrieved Contexts (from Pinecone):\")\n",
        "        print(full_context)\n",
        "\n",
        "        # Use the reader to extract the final answer\n",
        "        print(\"\\nUsing the reader to extract the exact answer...\")\n",
        "        final_answer = extract_answer(question, full_context)\n",
        "\n",
        "        print(\"\\nFinal Answer:\")\n",
        "        print(final_answer)\n",
        "    else:\n",
        "        print(\"No relevant context found.\")\n",
        "\n",
        "\n",
        "# --- Step 5: Clean Up ---\n",
        "# This command will delete the Pinecone index to free up your resources.\n",
        "try:\n",
        "    pc.delete_index(index_name)\n",
        "    print(f\"\\nIndex '{index_name}' deleted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError deleting index '{index_name}': {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "environment": {
      "name": "tf2-gpu.2-3.m65",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 333.240754,
      "end_time": "2021-04-15T21:12:11.363566",
      "environment_variables": {},
      "exception": null,
      "input_path": "/notebooks/question_answering/question_answering.ipynb",
      "output_path": "/notebooks/tmp/question_answering/question_answering.ipynb",
      "parameters": {},
      "start_time": "2021-04-15T21:06:38.122812",
      "version": "2.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
